{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22a774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sc\n",
    "# import sqlContext\n",
    "# import spark\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import array_contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eba8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.Dataframe()\n",
    "\n",
    "# for file in os.listdir(wdir):\n",
    "#     if file.endswith(\".bz2\"):\n",
    "#         #print(os.path.join(wdir, file))\n",
    "#         df_temp = sqlContext.read.json(os.path.join(wdir, file))\n",
    "#         df_temp2 = df_temp.filter(df_temp['text'].contains(\"3Landers\"))\n",
    "#         df_subset = df_temp2[['created_at', 'entities', 'text']]\n",
    "#         final_df.append(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fd7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pages = ['1','2','3','4','5','6']\n",
    "\n",
    "str_soup = ''\n",
    "\n",
    "for page in list_pages:\n",
    "\n",
    "    URL = \"https://www.coingecko.com/en/nft?page=\"\n",
    "    full_URL = URL + page\n",
    "    r = requests.get(full_URL)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "#print(soup.prettify())\n",
    "\n",
    "    for span in soup.find_all('td'):\n",
    "        str_soup += span.text[2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620ae019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_soup = ''\n",
    "\n",
    "# for page in list_pages:\n",
    "\n",
    "#     URL = \"https://nftpricefloor.com/boredApesYC\"\n",
    "#     #full_URL = URL + page\n",
    "#     r = requests.get(URL)\n",
    "\n",
    "#     soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "# print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422a2156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meebits',\n",
       " '13 ETH2.0%',\n",
       " ',600.0 ETH794.77 ETH,748.0',\n",
       " '0.1%',\n",
       " ',000.0dotdotdots',\n",
       " '129 ETH1.5%',\n",
       " '8.23 ETH406.89 ETH,671.0',\n",
       " '0.1%',\n",
       " '870.0CyberBrokers V3',\n",
       " '8269 ETH9.1%',\n",
       " '0 ETH485.61 ETH.0',\n",
       " '100.0%',\n",
       " '0VeeFriends',\n",
       " '.0 ETH3.8%',\n",
       " '4,335.0 ETH3.5 ETH,233.0',\n",
       " '.1%',\n",
       " ',255.0CloneX',\n",
       " '.7 ETH5.6%',\n",
       " '2,171.3 ETH4.94 ETH,461.0',\n",
       " '.2%',\n",
       " ',989.0Bored Ape Yacht Club',\n",
       " '.0 ETH6.7%',\n",
       " '0,000.0 ETH0.0 ETH,342.0',\n",
       " '.0%',\n",
       " ',000.0Mutant Ape Yacht Club',\n",
       " '.3 ETH7.6%',\n",
       " '5,201.1 ETH2.4 ETH1,837.0',\n",
       " '0.0%',\n",
       " ',987.0Azuki',\n",
       " '.19 ETH0.9%',\n",
       " '1,900.0 ETH3.6 ETH,525.0',\n",
       " '.0%',\n",
       " ',000.0Invisible Friends',\n",
       " '8 ETH10.8%',\n",
       " ',000.0 ETH8.37 ETH,161.0',\n",
       " '.0%',\n",
       " '000.0LAND',\n",
       " '94 ETH1.8%',\n",
       " '5,120.58 ETH4.16 ETH0,189.0',\n",
       " '.0%',\n",
       " '5,007.0CryptoPunks',\n",
       " '.95 ETH3.0%',\n",
       " '9,433.05 ETH6.45 ETH,406.0',\n",
       " '0.1%',\n",
       " '999.03Landers',\n",
       " '58888 ETH6.8%',\n",
       " ',888.8 ETH3.24 ETH,308.0',\n",
       " '.1%',\n",
       " ',000.0NFT Worlds',\n",
       " '0346 ETH9.4%',\n",
       " ',346.0 ETH3.11 ETH,560.0',\n",
       " '.0%',\n",
       " ',000.0Karafuru',\n",
       " '6 ETH2.1%',\n",
       " ',440.4 ETH7.23 ETH,656.0',\n",
       " '.0%',\n",
       " '554.0Metroverse',\n",
       " '85 ETH4.5%',\n",
       " ',500.0 ETH1.45 ETH41.0',\n",
       " '.2%',\n",
       " ',000.0Cool Cats',\n",
       " '469 ETH.7%',\n",
       " ',249.33 ETH2.18 ETH,297.0',\n",
       " '.3%',\n",
       " '941.0World of Women',\n",
       " '6 ETH8.7%',\n",
       " ',000.0 ETH6.99 ETH,111.0',\n",
       " '0.0%',\n",
       " ' ,000.0Doodles',\n",
       " '.19 ETH.0%',\n",
       " '1,900.0 ETH6.2 ETH,792.0',\n",
       " '.0%',\n",
       " ',000.0Hape Prime',\n",
       " '5 ETH0.2%',\n",
       " ',288.0 ETH8.49 ETH,749.0',\n",
       " '0.3%',\n",
       " '192.0JRNY NFT Club',\n",
       " '49 ETH6.5%',\n",
       " ',900.0 ETH.55 ETH,811.0',\n",
       " '0.1%',\n",
       " ',000.0Parallel',\n",
       " '0015 ETH4.4%',\n",
       " '79 ETH.32 ETH2,718.0',\n",
       " '.9%',\n",
       " '9.0Cool Pets',\n",
       " '299 ETH.7%',\n",
       " ',222.45 ETH.09 ETH0,291.0',\n",
       " '0.0%',\n",
       " ',647.0Decentral Games ICE Poker',\n",
       " '0 ETH0.8%',\n",
       " ',066.0 ETH.34 ETH,358.0',\n",
       " '.0%',\n",
       " ',533.0CyberKongz',\n",
       " '1 ETH.5%',\n",
       " ',500.0 ETH.53 ETH,409.0',\n",
       " '0.2%',\n",
       " '000.0Tubby Cats',\n",
       " '29 ETH10.2%',\n",
       " '800.0 ETH.41 ETH0,157.0',\n",
       " '0.3%',\n",
       " ',000.0FLUF World',\n",
       " '4999999999 ETH.9%',\n",
       " ',000.0 ETH.51 ETH,676.0',\n",
       " '.3%',\n",
       " ',000.0Pixelmon',\n",
       " '298 ETH.5%',\n",
       " '981.49 ETH.04 ETH,425.0',\n",
       " '0.4%',\n",
       " ',005.0iNFT Personality Pod',\n",
       " '57 ETH2.7%',\n",
       " '592.27 ETH.14 ETH,442.0',\n",
       " '1.3%',\n",
       " '811.0BYO Land',\n",
       " '74 ETH3.0%',\n",
       " ',497.44 ETH.02 ETH,957.0',\n",
       " '0.0%',\n",
       " ',056.0CatBlox Genesis',\n",
       " '275 ETH6.6%',\n",
       " '749.73 ETH.11 ETH,197.0',\n",
       " '0.5%',\n",
       " ' 999.0Boss Beauties',\n",
       " '979 ETH.1%',\n",
       " ',790.0 ETH.07 ETH,478.0',\n",
       " '0.1%',\n",
       " ',000.0Adam Bomb Squad',\n",
       " '609 ETH4.0%',\n",
       " ',224.39 ETH.85 ETH,654.0',\n",
       " '0.0%',\n",
       " ',999.0PUNKS Comic',\n",
       " '12 ETH13.2%',\n",
       " '784.48 ETH.71 ETH,371.0',\n",
       " '.0%',\n",
       " \",204.0Jenkins the Valet: The Writer's Room\",\n",
       " '5 ETH6.6%',\n",
       " ',413.0 ETH.84 ETH,842.0',\n",
       " '0.1%',\n",
       " '942.0OnChainMonkey',\n",
       " '9 ETH3.4%',\n",
       " ',050.0 ETH.47 ETH,000.0',\n",
       " '.1%',\n",
       " '500.0Smilesss',\n",
       " '989999 ETH0.4%',\n",
       " '648.63 ETH.22 ETH,532.0',\n",
       " '0.0%',\n",
       " '736.0Lazy Lions',\n",
       " '45 ETH.2%',\n",
       " ',614.55 ETH.38 ETH,100.0',\n",
       " '.0%',\n",
       " ',079.0merge',\n",
       " '189 ETH2.6%',\n",
       " '060.29 ETH.94 ETH,369.0',\n",
       " '0.0%',\n",
       " ',774.0FVCK_CRYSTAL//',\n",
       " '5 ETH.1%',\n",
       " ',422.5 ETH.97 ETH,146.0',\n",
       " '.1%',\n",
       " \"169.0Zipcy's SuperNormal\",\n",
       " '435 ETH2.0%',\n",
       " '866.28 ETH.4 ETH,679.0',\n",
       " '.0%',\n",
       " '888.0Treeverse Plots',\n",
       " '2 ETH4.7%',\n",
       " ',504.0 ETH.04 ETH,805.0',\n",
       " '.2%',\n",
       " ',420.0CrypToadz',\n",
       " '925 ETH2.1%',\n",
       " ',523.13 ETH.38 ETH,912.0',\n",
       " '.1%',\n",
       " '025.0Forgotten Runes Wizards Cult',\n",
       " '84 ETH1.5%',\n",
       " ',310.72 ETH.32 ETH,824.0',\n",
       " ' .1%',\n",
       " '408.0Crypto Unicorns',\n",
       " '32 ETH.7%',\n",
       " ',200.0 ETH.77 ETH19.0',\n",
       " '.0%',\n",
       " ',000.0Anonymice',\n",
       " '3 ETH.4%',\n",
       " ',000.0 ETH.81 ETH70.0',\n",
       " '0.3%',\n",
       " ',000.0Capsule House',\n",
       " '978 ETH14.0%',\n",
       " '780.0 ETH.66 ETH,796.0',\n",
       " '.1%',\n",
       " ',000.0Creature World',\n",
       " '82 ETH3.3%',\n",
       " '200.0 ETH.61 ETH,245.0',\n",
       " '.1%',\n",
       " ',000.0Lost Poets',\n",
       " '7 ETH04.7%',\n",
       " ',404.7 ETH.95 ETH,404.0',\n",
       " '0.1%',\n",
       " ',721.0Phanta Bear',\n",
       " '6975 ETH0.5%',\n",
       " '975.0 ETH.18 ETH,259.0',\n",
       " '0.1%',\n",
       " ',000.0Crypto Coven',\n",
       " '98 ETH5.7%',\n",
       " '565.78 ETH.88 ETH,810.0',\n",
       " '.0%',\n",
       " '761.0MekaVerse',\n",
       " '844 ETH.4%',\n",
       " '501.47 ETH.37 ETH,035.0',\n",
       " '.0%',\n",
       " '888.0The Currency',\n",
       " '81 ETH3.7%',\n",
       " '473.44 ETH.02 ETH,743.0',\n",
       " '.0%',\n",
       " '224.0Hashmasks',\n",
       " '62 ETH4.0%',\n",
       " ',149.4 ETH.1 ETH,146.0',\n",
       " '.0%',\n",
       " ',370.0Koala Intelligence Agency',\n",
       " '12 ETH2.6%',\n",
       " '200.0 ETH89 ETH,068.0',\n",
       " '0.6%',\n",
       " ',000.0Lives of Asuna',\n",
       " '205 ETH9.7%',\n",
       " '050.0 ETH82 ETH,836.0',\n",
       " '0.1%',\n",
       " ',000.0Chain Runners',\n",
       " '91 ETH18.0%',\n",
       " '100.0 ETH64 ETH,289.0',\n",
       " ' .0%',\n",
       " ',000.00N1 Force',\n",
       " '4 ETH3.2%',\n",
       " '110.8 ETH52 ETH,308.0',\n",
       " '0.0%',\n",
       " '777.08SIAN',\n",
       " '135 ETH0.9%',\n",
       " '199.88 ETH96 ETH,577.0',\n",
       " '.0%',\n",
       " '888.0LilHeroes',\n",
       " '875 ETH1.3%',\n",
       " '804.0 ETH27 ETH,130.0',\n",
       " '.2%',\n",
       " '776.0Sneaky Vampire Syndicate',\n",
       " '639 ETH.3%',\n",
       " '678.79 ETH27 ETH,374.0',\n",
       " '.1%',\n",
       " '887.0The Doge Pound',\n",
       " '04 ETH4.6%',\n",
       " ',400.0 ETH41 ETH,643.0',\n",
       " '.0%',\n",
       " ',000.0OCM Dessert',\n",
       " '499 ETH.2%',\n",
       " '5 ETH12 ETH,766.0',\n",
       " '0.2%',\n",
       " '0inbetweeners',\n",
       " '32 ETH18.1%',\n",
       " ' 448.64 ETH89 ETH,714.0',\n",
       " '0.1%',\n",
       " ',777.0the littles nft',\n",
       " '5 ETH15.7%',\n",
       " '000.0 ETH84 ETH,540.0',\n",
       " '0.1%',\n",
       " ',000.0PudgyPenguins',\n",
       " '8 ETH10.6%',\n",
       " '109.6 ETH38 ETH,398.0',\n",
       " '.0%',\n",
       " '887.0Timeless',\n",
       " '46 ETH6.7%',\n",
       " '333.66 ETH85 ETH,117.0',\n",
       " '0.1%',\n",
       " '421.0Samurai Saga',\n",
       " '198 ETH10.7%',\n",
       " '979.8 ETH52 ETH,546.0',\n",
       " '0.7%',\n",
       " '999.0SpacePunksClub',\n",
       " '0699 ETH13.3%',\n",
       " '8.65 ETH38 ETH,387.0',\n",
       " '0.0%',\n",
       " '995.0Divine Anarchy',\n",
       " '1677 ETH1.9%',\n",
       " '678.84 ETH23 ETH5,003.0',\n",
       " '0.1%',\n",
       " ',011.0Stoner Cats',\n",
       " '33 ETH.2%',\n",
       " '438.27 ETH19 ETH,066.0',\n",
       " '.0%',\n",
       " ',419.0GenesisApostle',\n",
       " '0 ETH5.5%',\n",
       " '256.0 ETH17 ETH,483.0',\n",
       " '0.0%',\n",
       " '256.0Skullx ',\n",
       " '025 ETH2.6%',\n",
       " '0.0 ETH43 ETH,552.0',\n",
       " '0.6%',\n",
       " ',000.0Cupcat NFT',\n",
       " '2498 ETH9.6%',\n",
       " '251.25 ETH36 ETH,807.0',\n",
       " '.0%',\n",
       " '009.0CryptoHoots Steampunk Parliament',\n",
       " '35 ETH6.2%',\n",
       " '5.0 ETH02 ETH04.0',\n",
       " '0.2%',\n",
       " '500.0MoonCats - Acclimated',\n",
       " '45 ETH9.0%',\n",
       " '532.0 ETH88 ETH,603.0',\n",
       " '.0%',\n",
       " ',960.0Genesis Mana',\n",
       " '26 ETH1.0%',\n",
       " '582.84 ETH77 ETH10.0',\n",
       " '.3%',\n",
       " '934.0Global Citizen Club',\n",
       " '07 ETH0.9%',\n",
       " '2.16 ETH74 ETH,803.0',\n",
       " '1.0%',\n",
       " '888.0The Vogu Collective',\n",
       " '145 ETH.4%',\n",
       " '127.67 ETH52 ETH,848.0',\n",
       " '0.2%',\n",
       " '777.0Tom Sachs Rocket Components',\n",
       " '2 ETH7.2%',\n",
       " '026.0 ETH5 ETH76.0',\n",
       " '.0%',\n",
       " '830.0Galaxy Eggs',\n",
       " '177 ETH.1%',\n",
       " '769.82 ETH48 ETH,956.0',\n",
       " '0.1%',\n",
       " '999.0DuskBreakers',\n",
       " '185 ETH.0%',\n",
       " '850.0 ETH42 ETH,877.0',\n",
       " '.1%',\n",
       " ',000.0HeavenComputer',\n",
       " '1485 ETH3.2%',\n",
       " '154.88 ETH4 ETH,897.0',\n",
       " '0.1%',\n",
       " '777.0Bloot',\n",
       " '04 ETH3.3%',\n",
       " '0.32 ETH3 ETH,955.0',\n",
       " '.0%',\n",
       " '008.0Citizens of Bulliever Island',\n",
       " '1469 ETH3.0%',\n",
       " '469.0 ETH97 ETH,298.0',\n",
       " '.2%',\n",
       " ',000.0PartyDegenerates',\n",
       " '19 ETH19.9%',\n",
       " '899.81 ETH65 ETH,646.0',\n",
       " '.0%',\n",
       " '999.0Corruption(s*)',\n",
       " '3 ETH.5%',\n",
       " '258.8 ETH56 ETH,533.0',\n",
       " '.0%',\n",
       " '196.0EtherLambos',\n",
       " '6 ETH0.0%',\n",
       " '560.0 ETH55 ETH55.0',\n",
       " '.0%',\n",
       " '600.0Loot',\n",
       " '85 ETH1.0%',\n",
       " ',391.15 ETH42 ETH,548.0',\n",
       " '.0%',\n",
       " '779.0BYOPills',\n",
       " '65 ETH0.9%',\n",
       " '0 ETH4 ETH.0',\n",
       " '',\n",
       " '0HeadDAO',\n",
       " '08 ETH.1%',\n",
       " '0.0 ETH34 ETH,052.0',\n",
       " '0.1%',\n",
       " ',000.0The Wicked Craniums',\n",
       " '10999999 ETH1.0%',\n",
       " '183.82 ETH27 ETH,307.0',\n",
       " '.1%',\n",
       " ',762.0Satoshibles',\n",
       " '1888 ETH1.5%',\n",
       " '4.0 ETH19 ETH,504.0',\n",
       " '.1%',\n",
       " '000.0X Rabbits Club',\n",
       " '099 ETH23.8%',\n",
       " '2.7 ETH13 ETH,357.0',\n",
       " '.0%',\n",
       " '502.0CHIBI DINOS',\n",
       " '1 ETH8.2%',\n",
       " '000.0 ETH13 ETH,246.0',\n",
       " '0.2%',\n",
       " ',000.0Metakrew',\n",
       " '0699 ETH.8%',\n",
       " '2.72 ETH04 ETH3,217.0',\n",
       " '.0%',\n",
       " '335.0The Superlative Secret Society',\n",
       " '1 ETH0.0%',\n",
       " '111.0 ETH98 ETH,967.0',\n",
       " '.0%',\n",
       " \",110.0Curious Addys' Trading Club\",\n",
       " '219 ETH4.8%',\n",
       " '095.0 ETH94 ETH,188.0',\n",
       " '.0%',\n",
       " '000.0Feudalz',\n",
       " '115 ETH17.5%',\n",
       " '1.06 ETH93 ETH32.0',\n",
       " '0.6%',\n",
       " '444.0Lil Pudgys',\n",
       " '0399 ETH.4%',\n",
       " '9.42 ETH83 ETH,478.0',\n",
       " '.0%',\n",
       " ',033.00Chibi Apes',\n",
       " '14 ETH6.3%',\n",
       " '0.0 ETH75 ETH,845.0',\n",
       " '0.1%',\n",
       " '000.01Squarmies',\n",
       " '034 ETH8.4%',\n",
       " '7.88 ETH74 ETH,586.0',\n",
       " '0.1%',\n",
       " '467.02Animetas',\n",
       " '173 ETH2.1%',\n",
       " '747.47 ETH68 ETH,155.0',\n",
       " '.0%',\n",
       " ',101.03Sipherian Inu',\n",
       " '229 ETH5.5%',\n",
       " '289.77 ETH66 ETH,759.0',\n",
       " '.0%',\n",
       " '999.04Cypher',\n",
       " '48999 ETH3.0%',\n",
       " '1.75 ETH59 ETH44.0',\n",
       " '.2%',\n",
       " '024.05The Sevens',\n",
       " '12 ETH8.9%',\n",
       " '0.0 ETH55 ETH,748.0',\n",
       " '.1%',\n",
       " '000.06Avastars',\n",
       " '08 ETH10.0%',\n",
       " '037.44 ETH53 ETH,030.0',\n",
       " '.1%',\n",
       " ',468.07CryptoZoo Base Egg',\n",
       " '141 ETH0.3%',\n",
       " '6.56 ETH5 ETH,459.0',\n",
       " '.1%',\n",
       " '160.08loomlock NFT',\n",
       " '655 ETH7.3%',\n",
       " '0 ETH49 ETH.0',\n",
       " '',\n",
       " '09Gauntlets',\n",
       " '07 ETH7.5%',\n",
       " '0.0 ETH34 ETH,828.0',\n",
       " '0.0%',\n",
       " ',000.00Bored Ape Comic',\n",
       " '03 ETH8.4%',\n",
       " '9.88 ETH26 ETH,694.0',\n",
       " '0.1%',\n",
       " '996.01JunkYardDogs ',\n",
       " '047 ETH3.0%',\n",
       " '4.92 ETH24 ETH,406.0',\n",
       " '0.1%',\n",
       " '977.02Tools of Rock',\n",
       " '033 ETH3.6%',\n",
       " '6.68 ETH23 ETH,253.0',\n",
       " '0.1%',\n",
       " '475.03Ethereals WTF',\n",
       " '0297 ETH8.1%',\n",
       " '6.65 ETH23 ETH,028.0',\n",
       " '0.1%',\n",
       " ',345.04Generative Masks',\n",
       " '06 ETH15.1%',\n",
       " '9.94 ETH22 ETH,453.0',\n",
       " '.0%',\n",
       " '999.05Apes of Space',\n",
       " '03 ETH15.1%',\n",
       " '0.0 ETH2 ETH,149.0',\n",
       " '.2%',\n",
       " ',000.06Ready Player Cat NFT',\n",
       " '08 ETH1.0%',\n",
       " '6.0 ETH17 ETH,093.0',\n",
       " '.0%',\n",
       " '950.07The Repus',\n",
       " '06 ETH18.6%',\n",
       " '8.22 ETH16 ETH,975.0',\n",
       " '.3%',\n",
       " '637.08Dope Shibas',\n",
       " '018 ETH1.0%',\n",
       " '8.18 ETH13 ETH,011.0',\n",
       " '.0%',\n",
       " '899.09Angry Apes United',\n",
       " '045 ETH1.5%',\n",
       " '9.96 ETH13 ETH,938.0',\n",
       " '0.0%',\n",
       " '888.00Space Poggers',\n",
       " '015 ETH12.6%',\n",
       " '6.63 ETH12 ETH,479.0',\n",
       " '0.1%',\n",
       " ',442.01Strange Attractors',\n",
       " ' 13 ETH1.0%',\n",
       " '.38 ETH1 ETH93.0',\n",
       " '.0%',\n",
       " '6.02Star Sailor Sibings',\n",
       " '046 ETH8.9%',\n",
       " '4.6 ETH1 ETH,811.0',\n",
       " '.0%',\n",
       " ',100.03Wizards & Dragons Game',\n",
       " '023 ETH18.6%',\n",
       " '061.89 ETH1 ETH,473.0',\n",
       " '.1%',\n",
       " ',169.04Ethermore',\n",
       " '0175 ETH8.8%',\n",
       " '2.5 ETH09 ETH,858.0',\n",
       " '.0%',\n",
       " ',000.05AlphaBetty Doodles',\n",
       " '035 ETH11.1%',\n",
       " '0.0 ETH08 ETH,524.0',\n",
       " '0.0%',\n",
       " ',000.06CryptoZombiez',\n",
       " '04 ETH15.7%',\n",
       " '2.2 ETH06 ETH27.0',\n",
       " '.7%',\n",
       " '555.07APE DAO REMIX!',\n",
       " '049 ETH10.1%',\n",
       " '1.95 ETH05 ETH,598.0',\n",
       " '0.1%',\n",
       " '550.08Non-Fungible Soup',\n",
       " '048 ETH1.0%',\n",
       " '.3 ETH05 ETH,133.0',\n",
       " '.0%',\n",
       " '048.09Bored Bananas',\n",
       " '029 ETH13.8%',\n",
       " '6.78 ETH04 ETH,846.0',\n",
       " '.1%',\n",
       " '027.00Baby Battle Bots G1',\n",
       " '025 ETH7.9%',\n",
       " '.45 ETH02 ETH,387.0',\n",
       " '0.1%',\n",
       " '498.01United Punks Union',\n",
       " '009 ETH1.0%',\n",
       " '.63 ETH01 ETH,195.0',\n",
       " '.0%',\n",
       " '625.02Singularity by AIIV',\n",
       " '11 ETH8.4%',\n",
       " '0.0 ETH0 ETH15.0',\n",
       " '0.2%',\n",
       " '000.03Derpy Birbs',\n",
       " '01 ETH1.0%',\n",
       " '.92 ETH0 ETH,837.0',\n",
       " '.0%',\n",
       " '192.04COVIDPunks',\n",
       " '013 ETH0.9%',\n",
       " '97 ETH0 ETH89.0',\n",
       " '.0%',\n",
       " '5.05IreneDAO',\n",
       " '499 ETH1.2%',\n",
       " '1.89 ETH0 ETH88.0',\n",
       " '.0%',\n",
       " '106.06Cometh Spaceships',\n",
       " '04 ETH1.0%',\n",
       " '4.32 ETH0 ETH89.0',\n",
       " '.0%',\n",
       " ',358.07Abstract Loot',\n",
       " '004 ETH1.0%',\n",
       " '.0 ETH0 ETH,871.0',\n",
       " '.0%',\n",
       " '000.08Extension Loot (xLoot)',\n",
       " '0099 ETH10.9%',\n",
       " '.01 ETH0 ETH,216.0',\n",
       " '.0%',\n",
       " '779.09Revenants by Alethea AI',\n",
       " '.0 ETH1.0%',\n",
       " '500.0 ETH0 ETH0.0',\n",
       " '.0%',\n",
       " '0.00Template',\n",
       " '0035 ETH63.5%',\n",
       " '.47 ETH0 ETH,075.0',\n",
       " '.0%',\n",
       " '992.01CryptoDragons Eggs',\n",
       " '05 ETH10.0%',\n",
       " '0.2 ETH0 ETH,372.0',\n",
       " '.1%',\n",
       " '804.02EtherLambos',\n",
       " '55 ETH5.8%',\n",
       " '480.0 ETH0 ETH55.0',\n",
       " '.0%',\n",
       " '600.03SquidDAONFT',\n",
       " '5 ETH0.9%',\n",
       " '7.5 ETH0 ETH6.0',\n",
       " '.0%',\n",
       " '1.04Pudgy Halloween',\n",
       " '013 ETH0.9%',\n",
       " '04 ETH0 ETH,326.0',\n",
       " '.0%',\n",
       " '05MixedFeelings ',\n",
       " '',\n",
       " '0 ',\n",
       " '06Where Did Macy Go?',\n",
       " '',\n",
       " '0 ',\n",
       " '07 Messari 2022 Theses ',\n",
       " '',\n",
       " '0 ',\n",
       " '08Re-Digitalization of Waves',\n",
       " '',\n",
       " '0 ',\n",
       " '09MetaHero',\n",
       " '82 ETH0.5%',\n",
       " ',845.28 ETH0 ETH17.0',\n",
       " '.0%',\n",
       " '504.00Fomoverse',\n",
       " '98 ETH0.9%',\n",
       " '911.32 ETH0 ETH60.0',\n",
       " '.0%',\n",
       " '234.01Spirit Orb Pets',\n",
       " '024 ETH1.0%',\n",
       " '.65 ETH0 ETH72.0',\n",
       " '.0%',\n",
       " '7.02CorruptedOG',\n",
       " '',\n",
       " '0 ',\n",
       " '03Neo Tokyo: Identities',\n",
       " '.49 ETH3.2%',\n",
       " ',263.29 ETH0 ETH7.0',\n",
       " '.0%',\n",
       " '021.04PEPEVERSE &#127760; &#128056;',\n",
       " '',\n",
       " '0 ',\n",
       " '05Incrementum',\n",
       " '015 ETH10.5%',\n",
       " '.0 ETH0 ETH,224.0',\n",
       " '.0%',\n",
       " '000.06Borpacasso',\n",
       " '0372 ETH1.0%',\n",
       " '.7 ETH0 ETH,145.0',\n",
       " '.0%',\n",
       " '250.07Light Super Bunnies',\n",
       " '02 ETH1.0%',\n",
       " '.14 ETH0 ETH,714.0',\n",
       " '.0%',\n",
       " '457.088 Bit Universe',\n",
       " '004 ETH1.0%',\n",
       " '0 ETH0 ETH.0',\n",
       " '',\n",
       " '09Lee Mullican',\n",
       " '0 ETH1.0%',\n",
       " '.0 ETH0 ETH5.0',\n",
       " '.0%',\n",
       " '.00MondrianNFT',\n",
       " '014 ETH1.0%',\n",
       " '.34 ETH0 ETH,829.0',\n",
       " '.0%',\n",
       " '096.01Night Kids',\n",
       " '00001 ETH99.9%',\n",
       " '06 ETH0 ETH,203.0',\n",
       " '.0%',\n",
       " '554.02Passive Apes',\n",
       " '000001 ETH1.0%',\n",
       " '0 ETH0 ETH,239.0',\n",
       " '.0%',\n",
       " '500.03Spookies',\n",
       " '01 ETH1.0%',\n",
       " '.88 ETH0 ETH,659.0',\n",
       " '.0%',\n",
       " '888.04Blitmap',\n",
       " '25 ETH4.5%',\n",
       " ',625.0 ETH0 ETH07.0',\n",
       " '.0%',\n",
       " '700.05Fortune',\n",
       " '75 ETH0.9%',\n",
       " '.0 ETH0 ETH13.0',\n",
       " '.0%',\n",
       " '06Dark Super Bunnies',\n",
       " '02 ETH1.0%',\n",
       " '.0 ETH0 ETH,737.0',\n",
       " '0.0%',\n",
       " '400.07Glue Factory Show',\n",
       " '044 ETH36.8%',\n",
       " '.73 ETH0 ETH.0',\n",
       " '.0%',\n",
       " '3.08StarryNift',\n",
       " 'TH',\n",
       " '0 ETH0 ETH58.0',\n",
       " '',\n",
       " '2.09Settlements',\n",
       " '048 ETH3.0%',\n",
       " '5.44 ETH0 ETH,577.0',\n",
       " '.0%',\n",
       " '905.00MOONDOGS ODYSSEY',\n",
       " '0125 ETH17.5%',\n",
       " '.5 ETH0 ETH,936.0',\n",
       " '.0%',\n",
       " '000.01Mojimon',\n",
       " 'TH',\n",
       " '0 ETH0 ETH.0',\n",
       " '',\n",
       " '0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_soup.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8d7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for x in str_soup.split(\"\\n\"):\n",
    "    if '%' not in x:\n",
    "        temp_list.append(x)\n",
    "        \n",
    "#temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff3876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f39cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list2 = []\n",
    "\n",
    "for x in temp_list:\n",
    "    temp_str = ''.join([i for i in x if not i.isdigit()])\n",
    "    temp_list2.append(temp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e11b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42bb9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list3 = []\n",
    "\n",
    "for x in temp_list2:\n",
    "    temp_str = ''.join(e for e in x if e.isalnum())\n",
    "    temp_list3.append(temp_str)\n",
    "\n",
    "#temp_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c7362d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_list_nft = []\n",
    "\n",
    "for x in temp_list3:\n",
    "    if x != 'ETHETH' and x != '' and x != 'TH' and x!= 'merge' and x!= 'Template' and x!= 'ETH' and x!= 'LAND': #too common in english language\n",
    "        final_list_nft.append(x)\n",
    "\n",
    "final_list_nft_12 = final_list_nft[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01dbf6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list_nft_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f12d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meebits',\n",
       " 'dotdotdots',\n",
       " 'CyberBrokersV',\n",
       " 'VeeFriends',\n",
       " 'CloneX',\n",
       " 'BoredApeYachtClub',\n",
       " 'MutantApeYachtClub',\n",
       " 'Azuki',\n",
       " 'InvisibleFriends',\n",
       " 'CryptoPunks',\n",
       " 'Landers',\n",
       " 'NFTWorlds']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_nft_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac27fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeriesAppend=[]\n",
    "\n",
    "# for nft_col in final_list_nft:\n",
    "#     series = df.filter(df.text.contains(nft_col))\n",
    "#     series_subset = series.select(\"created_at\", \"text\", \"entities\")\n",
    "\n",
    "#     SeriesAppend.append(series_subset)\n",
    "\n",
    "# df_series = reduce(DataFrame.unionAll, SeriesAppend)\n",
    "# print((df_series.count(), len(df_series.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea4822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d75800",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '/scratch/si699s007w22_class_root/si699s007w22_class/cgli/'\n",
    "#df = sqlContext.read.json(os.path.join(wdir,'tweets.2022-02-15.bz2'))\n",
    "\n",
    "\n",
    "\n",
    "#finished files: 'tweets.2022-02-15.bz2', 'tweets.2022-02-14.bz2, 'tweets.2022-02-12.bz2', 'tweets.2022-02-11.bz2', \n",
    "    #'tweets.2022-02-10.bz2','tweets.2022-02-09.bz2', 'tweets.2022-02-08.bz2'\n",
    "#corrupted files: 'tweets.2022-02-13.bz2',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecad0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# SeriesAppend=[]\n",
    "\n",
    "# f = 'tweets.2022-02-12.bz2' #bzip2 -d file.bz2 - unzip and see if I can fix files: 2022-02-13\n",
    "\n",
    "# #df = spark.read.json(os.path.join(wdir,f))\n",
    "# df = spark.read.load(os.path.join(wdir,f), format=\"json\")\n",
    "\n",
    "# print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1584609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for nft_col in final_list_nft:\n",
    "#     df_filtered = df.filter(df.text.contains(nft_col))\n",
    "#     df_subset = df_filtered.select(\"created_at\", \"text\", \"entities\")\n",
    "\n",
    "#     SeriesAppend.append(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b298276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #df_series = unionAll(SeriesAppend)\n",
    "# df_series = reduce(DataFrame.unionAll, SeriesAppend)\n",
    "# print((df_series.count(), len(df_series.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c21a94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21581178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cwd = os.getcwd()\n",
    "#cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac4c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #df_series.toPandas().to_csv('data_2022-02-12.csv')\n",
    "# #df_series.write.csv('data_2022-02-12.csv')\n",
    "\n",
    "# df_series_subset = df_series.select(\"created_at\", \"text\")\n",
    "\n",
    "# df_series_subset.write.format(\"csv\").save(\"/home/cgli/data_2022-02-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba30d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de160cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test if file is corrupted\n",
    "# df = spark.read.load(os.path.join(wdir,'tweets.2022-02-08.bz2'), format=\"json\")\n",
    "# print((df.count(), len(df.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d9dd0",
   "metadata": {},
   "source": [
    "**test runtime with 4 bz2 files --> 1 h 44min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "470624e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_f = [\n",
    "# 'tweets.2021-08-24.bz2', \n",
    "# 'tweets.2021-08-26.bz2', \n",
    "# 'tweets.2021-08-28.bz2', \n",
    "# 'tweets.2021-08-30.bz2', \n",
    "# 'tweets.2021-09-02.bz2', \n",
    "# 'tweets.2021-09-04.bz2', \n",
    "# 'tweets.2021-09-06.bz2',\n",
    "# 'tweets.2021-09-08.bz2',\n",
    "# 'tweets.2021-09-10.bz2',\n",
    "# 'tweets.2021-09-12.bz2',    \n",
    "# 'tweets.2021-09-14.bz2', \n",
    "# 'tweets.2021-09-16.bz2', \n",
    "# 'tweets.2021-09-18.bz2',\n",
    "# 'tweets.2021-09-20.bz2',  \n",
    "# 'tweets.2021-09-22.bz2',\n",
    "# 'tweets.2021-09-24.bz2',  \n",
    "# 'tweets.2021-09-26.bz2', \n",
    "# 'tweets.2021-09-28.bz2',\n",
    "# 'tweets.2021-09-30.bz2',  \n",
    "# 'tweets.2021-10-02.bz2', \n",
    "# 'tweets.2021-10-05.bz2',\n",
    "# 'tweets.2021-10-07.bz2',  \n",
    "# 'tweets.2021-10-09.bz2', \n",
    "# 'tweets.2021-10-11.bz2', \n",
    "# 'tweets.2021-10-13.bz2',\n",
    "# 'tweets.2021-10-15.bz2',\n",
    "# 'tweets.2021-10-17.bz2',   \n",
    "# 'tweets.2021-10-19.bz2', \n",
    "# 'tweets.2021-10-21.bz2', \n",
    "# 'tweets.2021-10-23.bz2', \n",
    "# 'tweets.2021-10-25.bz2', \n",
    "# 'tweets.2021-10-27.bz2', \n",
    "# 'tweets.2021-10-29.bz2', \n",
    "# 'tweets.2021-10-31.bz2', \n",
    "'tweets.2021-11-02.bz2', \n",
    "'tweets.2021-11-04.bz2', \n",
    "'tweets.2021-11-06.bz2', \n",
    "'tweets.2021-11-08.bz2', \n",
    "'tweets.2021-11-10.bz2', \n",
    "'tweets.2021-11-12.bz2', \n",
    "'tweets.2021-11-14.bz2', \n",
    "'tweets.2021-11-16.bz2', \n",
    "'tweets.2021-11-18.bz2', \n",
    "'tweets.2021-11-20.bz2', \n",
    "'tweets.2021-11-22.bz2', \n",
    "'tweets.2021-11-24.bz2', \n",
    "'tweets.2021-11-26.bz2', \n",
    "'tweets.2021-11-28.bz2', \n",
    "'tweets.2021-11-30.bz2', \n",
    "'tweets.2021-12-02.bz2', \n",
    "'tweets.2021-12-04.bz2', \n",
    "'tweets.2021-12-06.bz2', \n",
    "#'tweets.2021-12-08.bz2', #corrupted\n",
    "'tweets.2021-12-14.bz2', \n",
    "'tweets.2021-12-16.bz2', \n",
    "'tweets.2021-12-18.bz2', \n",
    "'tweets.2021-12-20.bz2', \n",
    "'tweets.2021-12-22.bz2', \n",
    "'tweets.2021-12-24.bz2', \n",
    "'tweets.2021-12-26.bz2', \n",
    "'tweets.2021-12-28.bz2', \n",
    "'tweets.2021-12-30.bz2' \n",
    "# 'tweets.2022-01-02.bz2', #done\n",
    "# 'tweets.2022-01-04.bz2', #done\n",
    "# 'tweets.2022-01-06.bz2', #done\n",
    "# 'tweets.2022-01-08.bz2', #done\n",
    "# 'tweets.2022-01-10.bz2', #done\n",
    "# 'tweets.2022-01-12.bz2', #done\n",
    "# 'tweets.2022-01-14.bz2', #done\n",
    "# 'tweets.2022-01-16.bz2', #done\n",
    "# 'tweets.2022-01-18.bz2', #done\n",
    "# 'tweets.2022-01-25.bz2', #done\n",
    "# 'tweets.2022-01-29.bz2'#done\n",
    "# 'tweets.2022-02-01.bz2', #done\n",
    "# 'tweets.2022-02-03.bz2', #done\n",
    "# 'tweets.2022-02-05.bz2', #done\n",
    "# 'tweets.2022-02-07.bz2', #done\n",
    "# 'tweets.2022-02-09.bz2', #done\n",
    "# 'tweets.2022-02-12.bz2', #done\n",
    "#  'tweets.2022-02-14.bz2', #done\n",
    "#  'tweets.2022-02-16.bz2' #done\n",
    "# 'tweets.2022-02-18.bz2' #corrupted\n",
    "# 'tweets.2022-02-20.bz2', #done\n",
    "# 'tweets.2022-02-22.bz2', #done\n",
    "# 'tweets.2022-02-24.bz2', #done\n",
    "# 'tweets.2022-02-26.bz2', #done\n",
    "# 'tweets.2022-02-28.bz2'  #done\n",
    "] \n",
    "len(list_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4c56b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.load(os.path.join(wdir,'tweets.2021-12-30.bz2'), format=\"json\")\n",
    "# print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b0d59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import pyspark.sql.functions as f\n",
    "\n",
    "# df_filtered = df.where(f.final_list_nft.isin(col(\"text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2863ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((df_filtered.count(), len(df_filtered.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ebcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d11abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08fb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "047c331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 239 ms, sys: 103 ms, total: 342 ms\n",
      "Wall time: 26min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_df=[]\n",
    "\n",
    "for f in list_f:\n",
    "    df = spark.read.load(os.path.join(wdir,f), format=\"json\")\n",
    "    \n",
    "    for nft_col in final_list_nft_12:\n",
    "        df_filtered = df.filter(df.text.contains(nft_col))\n",
    "        df_subset = df_filtered.select(\"created_at\", \"text\")\n",
    "\n",
    "        list_df.append(df_subset)\n",
    "        \n",
    "df_series = reduce(DataFrame.unionAll, list_df)\n",
    "\n",
    "df_series.write.format(\"csv\").save(\"/home/cgli/data_2021_December_02_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61585053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop running notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d7bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab6e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec5128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59b018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483f358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6343a653",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 7) (10.164.8.67 executor 0): java.io.IOException: unexpected end of stream\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:971)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:506)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:335)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:425)\n\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:491)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:186)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:94)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:58)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: unexpected end of stream\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:971)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:506)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:335)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:425)\n\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:491)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:186)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/sw/dis/centos7/spark/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/dis/centos7/spark/spark-3.1.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/dis/centos7/spark/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/dis/centos7/spark/spark-3.1.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 7) (10.164.8.67 executor 0): java.io.IOException: unexpected end of stream\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:971)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:506)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:335)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:425)\n\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:491)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:186)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:94)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:58)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: unexpected end of stream\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:971)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:506)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:335)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:425)\n\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:491)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:186)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SeriesAppend=[]\n",
    "\n",
    "for f in list_files:\n",
    "    df = sqlContext.read.json(os.path.join(wdir,f))\n",
    "    \n",
    "    for nft_col in final_list_nft:\n",
    "        series = df.filter(df.text.contains(nft_col))\n",
    "        series_subset = series.select(\"created_at\", \"text\", \"entities\")\n",
    "\n",
    "        SeriesAppend.append(series_subset)\n",
    "\n",
    "df_series = reduce(DataFrame.unionAll, SeriesAppend)\n",
    "print((df_series.count(), len(df_series.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce18d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f2d512",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d5dfa0ee5fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_series' is not defined"
     ]
    }
   ],
   "source": [
    "df_series.toPandas().to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b3c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ae73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefeaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877be917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac858440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d34d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.filter(df.text.contains(\"3Landers\") | \n",
    "                df.text.contains(\"Bored Ape Yacht Club\") | \n",
    "                df.text.contains(\"World of Women\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = df.filter(any(nft in df['text'] for nft in nft_list))\n",
    "\n",
    "#df['text'].contains(\"3Landers\")\n",
    "#email_contains_service = any(email_service in user_email for email_service in email_services)\n",
    "\n",
    "#df_temp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[['created_at', 'entities', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c67889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35147960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['text'].contains(\"3Landers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = df.select('created_at','text')\n",
    "tweet.printSchema()\n",
    "tweet.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "arr2 = df.select(explode('entities.user_mentions.name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from opensea import get_collection_stats\n",
    "\n",
    "# # This will return a CollectionStats Object\n",
    "# stats = get_collection_stats(collection=\"azuki\")\n",
    "\n",
    "# print(stats.count)\n",
    "# print(stats.num_owners)\n",
    "# print(stats.floor_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51747177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pip install nfts\n",
    "# import nfts.dataset\n",
    "# import os\n",
    "# import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3288a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/ethereum-nfts\")\n",
    "# DATASET_PATH = \"/kaggle/input/ethereum-nfts/nfts.sqlite\"\n",
    "# ds = nfts.dataset.FromSQLite(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opensea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from opensea import get_assets\n",
    "\n",
    "# # This will return a list of assets which you can iterate and get the needed data\n",
    "# asset_list = get_assets(limit=10, verified_only=False)\n",
    "\n",
    "# asset = asset_list[0] # Get the first asset obejct from the list\n",
    "\n",
    "# print(asset.name)\n",
    "# print(asset.description)\n",
    "# print(asset.asset_url)\n",
    "# print(asset.get_floor_price()) # Floor price of the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from opensea import get_bundles\n",
    "\n",
    "# # This will return a list of assets which you can iterate and get the needed data\n",
    "# bundles_list = get_bundles(limit=10)\n",
    "\n",
    "# bundle = bundles_list[0] # Get the first asset obejct from the list\n",
    "\n",
    "# print(bundle.slug)\n",
    "# print(bundle.assets[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from opensea import get_collection_stats\n",
    "\n",
    "# # This will return a CollectionStats Object\n",
    "# stats = get_collection_stats(collection=\"doodles-official\")\n",
    "\n",
    "# print(stats.count)\n",
    "# print(stats.num_owners)\n",
    "# print(stats.floor_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d636ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
